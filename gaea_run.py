import asyncio
import sys
import time
import cloudscraper

PING_URL = 'https://api.aigaea.net/api/network/ping'
IP_URL = 'https://api.aigaea.net/api/network/ip'
SCORE_URL = 'https://api.aigaea.net/api/earn/info'
DAILY_CHECK_URL = 'https://api.aigaea.net/api/mission/complete-mission'
from loguru import logger

logger.remove()
logger.add(sys.stdout, format="<g>{time:YYYY-MM-DD HH:mm:ss:SSS}</g> | <c>{level}</c> | <level>{message}</level>")
# logger.add(
#     "logs/file_{time}.log",  # æ–‡ä»¶è·¯å¾„ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºç›®å½•
#     rotation="500 MB",  # æ—¥å¿—æ–‡ä»¶å¤§å°è¾¾åˆ°500MBæ—¶ä¼šè‡ªåŠ¨æ–°å»ºæ–‡ä»¶
#     encoding="utf-8",  # è®¾ç½®ç¼–ç 
#     enqueue=True,  # å¼‚æ­¥å†™å…¥
#     retention="10 days",  # ä¿ç•™10å¤©çš„æ—¥å¿—
#     format="{time:YYYY-MM-DD HH:mm:ss:SSS} | {level} | {message}",  # æ–‡ä»¶ä¸­çš„æ—¥å¿—æ ¼å¼
#     level="DEBUG"  # æ—¥å¿—çº§åˆ«
# )


class ScraperReq:
    def __init__(self, proxy: dict, header: dict):
        self.scraper = cloudscraper.create_scraper(browser={
            'browser': 'chrome',
            'platform': 'windows',
            'mobile': False,
        })
        self.proxy: dict = proxy
        self.header: dict = header

    def post_req(self, url, req_json, req_param=None):
        # logger.info(self.header)
        # logger.info(req_json)
        return self.scraper.post(url=url, headers=self.header, json=req_json, proxies=self.proxy, params=req_param)

    async def post_async(self, url, req_param=None, req_json=None):
        return await asyncio.to_thread(self.post_req, url, req_json, req_param)

    def get_req(self, url, req_param):
        return self.scraper.get(url=url, headers=self.header, params=req_param, proxies=self.proxy)

    async def get_async(self, url, req_param=None, req_json=None):
        return await asyncio.to_thread(self.get_req, url, req_param)


class Geae:
    def __init__(self, account: dict, scraper: ScraperReq, check_score_scraper: ScraperReq):
        self.account: dict = account
        self.scraper = scraper
        self.country = ''
        self.proxy_res = ''
        self.check_score_scraper = check_score_scraper
        self.time_ = time.time()

    async def get_proxy_res(self):
        res = await self.scraper.get_async(url='http://ip-api.com/json', req_param=None)
        self.proxy_res = res.json()['query']
        self.country = res.json()['country']

    async def ping(self):
        req_json = {"uid": self.account['uid'], "browser_id": self.account['browser_id'], "timestamp": int(time.time()),
                    "version": "1.0.1"}
        res = await self.scraper.post_async(PING_URL, req_json=req_json)
        return res

    async def ip(self):
        res = await self.scraper.get_async(IP_URL)
        print(res.text)

    async def check_score(self):
        """
        {
            "code": 200,
            "success": true,
            "msg": "Success",
            "data": {
                "total_total": 1569.51,
                "today_total": 477.08
            }
        }
        """
        res = await self.check_score_scraper.get_async(SCORE_URL)
        # print(res.text)
        if res.json()['code'] == 200:
            logger.info(
                f'SCORE: è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]} ä»£ç†ipï¼š{self.proxy_res}, ä»£ç†å›½å®¶ï¼š{self.country}, åˆ†æ•°ï¼š{res.json()["data"]["total_total"]}')
        else:
            logger.error(
                f'SCORE: è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]} ä»£ç†ipï¼š{self.proxy_res}, ä»£ç†å›½å®¶ï¼š{self.country}, é”™è¯¯ï¼š{res.json()["msg"]}')

    async def check_daily_statistic(self):
        """æ£€æŸ¥æ¯æ—¥ç»Ÿè®¡ä¿¡æ¯åˆ¤æ–­æ˜¯å¦éœ€è¦ç­¾åˆ°"""
        try:
            res = await self.check_score_scraper.get_async('https://api.aigaea.net/api/earn/daily-statistic')
            data = res.json().get('data', [])
            
            # ä¿®æ”¹1ï¼šä½¿ç”¨UTCæ—¶é—´
            today = time.strftime("%d/%m/%Y", time.gmtime())  # UTCæ—¶é—´
            
            if not data:
                logger.info(f'è´¦å·ï¼š{self.account["email"]}æ¯æ—¥ç»Ÿè®¡æ•°æ®ä¸ºç©ºï¼Œéœ€è¦ç­¾åˆ°')
                return True
                
            first_entry = data[0]
            # ä¿®æ”¹2ï¼šæ·»åŠ network_pointsæ—¥å¿—
            if first_entry['date'] == today and first_entry['checkin_points'] == 0:
                logger.info(f'è´¦å·ï¼š{self.account["email"]} ä»Šæ—¥{first_entry["date"]}å°šæœªç­¾åˆ°ï¼Œå½“å‰network_pointsï¼š{first_entry.get("network_points", 0)}')
                return True
                
            return False
        except Exception as e:
            logger.error(f'è´¦å·ï¼š{self.account["email"]}æ£€æŸ¥æ¯æ—¥ç»Ÿè®¡å¤±è´¥: {e}')
            return False

    async def loop_task(self):
        try:
            await self.get_proxy_res()
        except Exception as e:
            return
            
        while True:
            try:
                await self.get_uid()
                
                # ä¿®æ”¹åçš„ç­¾åˆ°è§¦å‘æ¡ä»¶
                if await self.check_daily_statistic():
                    await self.daily_check_in()
                    self.time_ = time.time()  # é‡ç½®è®¡æ—¶
                    
                res = await self.ping()
                logger.info(
                    f'PING: ipåˆ†æ•°ï¼š{res.json()["data"]["score"]},è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]} ä»£ç†ipï¼š{self.proxy_res}, ä»£ç†å›½å®¶ï¼š{self.country}')
            except Exception as e:
                logger.error(f'PINGé”™è¯¯: {e}')
            try:
                await self.check_score()
            except Exception as e:
                logger.error(
                    f'IP: è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]} ä»£ç†ipï¼š{self.proxy_res}, ä»£ç†å›½å®¶ï¼š{self.country}, é”™è¯¯ï¼š{e}')
            await asyncio.sleep(10 * 60)

    async def daily_check_in(self):
        req_json = {"mission_id": "1"}
        res = await self.scraper.post_async(url=DAILY_CHECK_URL, req_param=None, req_json=req_json)
        if res.json()['code'] == 200:
            # ä¿®æ”¹3ï¼šç­¾åˆ°æˆåŠŸæ—¥å¿—æ·»åŠ network_points
            logger.info(
                f'DAILY-CHECK: å®Œæˆï¼Œè´¦å·ï¼š{self.account["email"]}, network_pointsï¼š{res.json().get("data", {}).get("network_points", "N/A")}')
        else:
            logger.info(
                f'DAILY-CHECK: å¤±è´¥ï¼Œè´¦å·ï¼š{self.account["email"]}')

    async def get_uid(self):
        try:
            session_url = 'https://api.aigaea.net/api/auth/session'

            res = await self.check_score_scraper.post_async(session_url)
            uid = res.json()['data']['uid']
            self.account.update({'uid': uid})
            logger.info(f'è·å–uidæˆåŠŸ:è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]}, uid: {uid}')
        except Exception as e:
            logger.error(
                f'è·å–uidå¤±è´¥:è´¦å·ï¼š{self.account["email"]}, socksä¿¡æ¯ï¼š{self.account["proxy"]} , {e}')
            raise Exception("è·å–uidå¤±è´¥", e)
        pass


async def run_gaea(account: dict):
    header = {
        'Authorization': f'Bearer {account["token"]}',
        'Content-Type': 'application/json',
        'Origin': 'chrome-extension://cpjicfogbgognnifjgmenmaldnmeeeib',
        'Priority': 'u=1, i',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'none'
    }

    proxy_dict = {
        'http': account['proxy'],
        'https': account['proxy'],
    }

    scraper = ScraperReq(proxy_dict, header)

    check_score_header = {
        'Authorization': f'Bearer {account["token"]}',
        'Content-Type': 'application/json',
        'Origin': 'https://app.aigaea.net',
        'Priority': 'u=1, i',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'same-site'
    }

    check_score_scraper = ScraperReq(proxy_dict, check_score_header)
    gaea = Geae(account, scraper, check_score_scraper)
    await gaea.loop_task()


async def main():
    """
    account æ–‡ä»¶æ ¼å¼ï¼šemail----password----browser_id----proxy----token
    """
    accounts = []
    with open('./account', 'r') as f:
        for l in f.readlines():
            email, password, browser_id, proxy, token = l.strip().split('----')
            accounts.append({
                'email': email,
                'password': password,
                'browser_id': browser_id,
                'proxy': proxy,
                'token': token
            })
    tasks = [run_gaea(account) for account in accounts]
    await asyncio.gather(*tasks)


if __name__ == '__main__':
    logger.info('ğŸš€ [ILSH] DAWN v1.0 | Airdrop Campaign Live')
    logger.info('ğŸŒ ILSH Community: t.me/ilsh_auto')
    logger.info('ğŸ¦ X(Twitter): https://x.com/hashlmBrian')
    logger.info('â˜• Pay meCoffeï¼šUSDTï¼ˆTRC20ï¼‰:TAiGnbo2isJYvPmNuJ4t5kAyvZPvAmBLch')
    asyncio.run(main())
